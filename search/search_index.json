{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A complete 16S amplicon sequence analysis workflow","text":"<p>This website provides a detailed explaination of automating the analysis of 16S amplicon read sequences using Snakemake. A complete analysis of 16S amplicon read sequences consists of several steps, ranging from quality control, taxonomy generation to statistical analysis. These steps must be executed in a specific order. </p> <p>To automate the execution of all analysis steps for 16S amplicon data, we use a workflow manager such as Snakemake. In this tutorial, we will demonstrate how to use Snakemake to create a workflow that reads raw 16S amplicon sequences and produces relative abundace data for subsequent analysis. </p>"},{"location":"dataset/","title":"16S Amplicon Sequence Reads","text":"<p>For this tutorial, we use 16S amplicon read sequences obtained from faecal sample from three patient groups:Healthy, Adenoma, CRC (Zackular et al., 2014). The authors investigated gut microbiome potential for predicting cases with colorectal cancer. They used 16S amplicon sequences for the analysis. You can read more about the study here. </p>"},{"location":"dataset/#data-files","title":"Data files","text":"<p>You can download the 16S amplicon sequence reads and metadata file from the following links.</p> <ul> <li>Raw Fastq Paired Reads</li> <li>Metadata</li> </ul>"},{"location":"dataset/#references","title":"References","text":"<ol> <li>Zackular, J. P., Rogers, M. A., Ruffin IV, M. T., &amp; Schloss, P. D. (2014). The human gut microbiome as a screening tool for colorectal cancer. Cancer prevention research, 7(11), 1112-1121.</li> </ol>"},{"location":"execution/","title":"Execution","text":"<p>This section guides you through running the workflow to compute relative abundance metrics for various microbial species.</p>"},{"location":"execution/#clone-the-repository","title":"Clone the repository","text":"<p>Clone the GitHub repository to access the scripts and configuration files needed for the workflow: <pre><code>git clone https://github.com/pankajchejara23/16S-workflow\n</code></pre></p>"},{"location":"execution/#download-dataset","title":"Download dataset","text":"<p>Navigate to the cloned repository and download the dataset using the provided script: <pre><code>cd 16S-workflow\nsh ./scripts/downloader.sh\n</code></pre></p>"},{"location":"execution/#create-processing-environment","title":"Create processing environment","text":"<p>Set up the required Conda environment using the environment file: <pre><code>conda env create -f ../envs/environment.yml\nconda activate qiime2_snakemake\n</code></pre></p>"},{"location":"execution/#configure-snakemake-workflow","title":"Configure Snakemake workflow","text":"<p>The <code>config.yaml</code> file is preconfigured for this workflow. If needed, edit the file to customize parameters for your specific use case.</p>"},{"location":"execution/#execute","title":"Execute","text":"<p>Run the Snakemake workflow using the following command: <pre><code>Snakemake --cores 4 \n</code></pre></p>"},{"location":"setup/","title":"Setup","text":"<p>We start by setting up our machine with required tools and packages to get it ready for executing the workflow. We primarily need the following tools</p> <ul> <li>Miniconda</li> <li>Qiime2 (2024.10)</li> <li>Snakemake</li> </ul> <p>To speed up the installation, we will use an <code>environment.yml</code> file exported from a conda environment that successfully executed the workflow. You can download the file from environment. </p> <p>You can create a new environment using the following command. This will create a environment with name qiime2_snakemake.</p> <pre><code>conda env create -f ../envs/environment.yml\n</code></pre> <p>Note</p> <p>To execute this command you need to have either Miniconda or Anaconda installed on your system. </p>"},{"location":"workflow/","title":"Snakemake workflow","text":"<p>This section provides detailed explanation for building a complete workflow using Snakemake. For our workflow, we will do the following.</p> <ol> <li>Configuration file: We will create a <code>config.yml</code> file which will contain information in <code>key</code>:<code>value</code> format about the project such as input directory of raw reads file and paths to reference databases.  Storing this information in a seperate configuration file allows us to reuse our workflow for different projects. </li> <li>Write rules for each step: We will write rules following Snakemake syntax for each step. These rules basically invovles specifying input file, output file, command to execute and other parameters. </li> <li>Write <code>rule_all</code>: By default Snakemake executes the first rule in the workflow. If the input files required for that rule are generated by other rules, those rules are also executed as well. To ensure that Snakemake executes all rules, we will write a <code>rule_all</code> that specifies all output files from all rules as inputs.</li> </ol> <p>Don't worry if it does not make sense now -- we will go through this in detail in the following sections.</p>"},{"location":"workflow/#references","title":"References","text":"<ol> <li>K\u00f6ster, J., &amp; Rahmann, S. (2012). Snakemake\u2014a scalable bioinformatics workflow engine. Bioinformatics, 28(19), 2520-2522.</li> </ol>"},{"location":"workflow/adapters/","title":"Adapter removal","text":"<p>This step removes adapter sequences from the sequencing reads. Adapter sequences are short DNA fragments added during library preparation to facilitate sequencing and indexing. Removing these adapters, along with other unwanted sequences such as primers, improves data quality and ensures accurate downstream analysis.</p> <p>To achieve this, the Cutadapt plugin in Qiime2 is used. Cutadapt efficiently trims adapter sequences, filters low-quality reads, and removes unwanted fragments, ensuring cleaner and more reliable sequencing data for further processing.</p> <pre><code>##########################################################\n#                 REMOVE PRIMERS\n##########################################################\n\nrule rm_primers:\n  input:\n    q2_import = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux.qza\"\n  output:\n    q2_primerRM = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ +  \"-PE-demux-noprimer.qza\"\n  log:\n     SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_primer_q2.log\"\n\n  shell:\n    \"\"\"qiime cutadapt trim-paired \\\n       --i-demultiplexed-sequences {input.q2_import} \\\n       --p-front-f {config[primerF]} \\\n       --p-front-r {config[primerR]} \\\n       --p-error-rate {config[primer_err]} \\\n       --p-overlap {config[primer_overlap]} \\\n       --o-trimmed-sequences {output.q2_primerRM}\n       \"\"\"\n</code></pre>"},{"location":"workflow/adapters/#references","title":"References","text":"<ol> <li>Martin, M. (2011). Cutadapt removes adapter sequences from high-throughput sequencing reads. EMBnet. journal, 17(1), 10-12.</li> </ol>"},{"location":"workflow/config/","title":"Config file","text":"<p>A configuration file specifies all project-related inforamation in a yml file. This configuration file is all which needs to be updated in order to execute the workflow for another project. This makes bioinfomatician's file easier. Just think chaning your workflow every time you run it for a different project. In this cases, there is a high likelihood of human error. </p> <p>To avoid this situation, all project-related information are kept in a yml file. Our workflow uses the following configuration file.</p> <p>The table provides information on each key and thier meaning. </p> Key Description project Name of the project; serves as an identifier for the analysis. scratch Path to the scratch or working directory where outputs will be stored. raw_data Directory containing raw input data files (e.g., raw FASTQ files). outputDIR Directory where final pipeline outputs will be stored. metadata Path to the QIIME2 metadata file containing sample information such as groups or treatments. tmp_dir Temporary directory for QIIME2 operations (e.g., caching files during execution). suffix File suffix used to identify the raw FASTQ files (e.g., <code>_001.fastq.gz</code>). prefix File prefix used in raw FASTQ file naming (e.g., <code>_L001_</code>). r1_suf Identifier used for forward read files (R1) in paired-end sequencing data. r2_suf Identifier used for reverse read files (R2) in paired-end sequencing data. threads Number of threads to use for multi-threaded tools like Trimmomatic. trimm_params Parameters for trimming raw sequences with Trimmomatic (e.g., cropping sequences to a length of 200). primerF Forward primer sequence used for amplifying the 16S rRNA region. primerR Reverse primer sequence used for amplifying the 16S rRNA region. primer_err Maximum allowable primer error rate during sequence matching. primer_overlap Minimum overlap between primer and sequence for matching. database Path to the QIIME2 reference database sequences file (e.g., SILVA 138.1). database_classifier Path to the QIIME2 pre-trained classifier for taxonomic classification. database_tax Path to the QIIME2 taxonomy file associated with the reference database. truncation_err Maximum allowable error for sequence truncation in DADA2 (used for quality filtering). truncation_len-f Length to truncate forward reads (R1) in DADA2 analysis. truncation_len-r Length to truncate reverse reads (R2) in DADA2 analysis. quality_err Maximum allowable quality error for sequence processing in DADA2. sampling_depth Depth of subsampling used for diversity analysis to ensure consistency across samples. <p>The following is the configuration file used for the workflow.</p> <pre><code># Basic setup\nproject: crc_microbiome\nscratch: /Users/pankaj/Documents/Metrosert/public_dataset/zakular2014\nraw_data: partial_raw_data\noutputDIR: full_pipeline\nmanifest: manifest_partial.csv\nmetadata: metadata_partial.tsv\n\n# Tmp direcotry for qiime2\ntmp_dir: ./\n\n# Fastq file naming config\nsuffix: _001.fastq.gz\nprefix: _L001_\nr1_suf: R1\nr2_suf: R2\n\n# Trimmomatic config\nthreads: 20\ntrimm_params: CROP:200\n\n## 16S adapters from Zackular et al., 2014\nprimerF: GTGCCAGCMGCCGCGGTAA\nprimerR: GGACTACHVGGGTWTCTAAT\nprimer_err: 0.4\nprimer_overlap: 3\n\n## Reference database\ndatabase: silva-138.1-ssu-nr99-seqs-515f-806r.qza\ndatabase_classifier: silva_classifier.qza\ndatabase_tax: silva-138.1-ssu-nr99-tax.qza\n\n## DADA2 - ASV flags\ntruncation_err: 2\ntruncation_len-f: 150\ntruncation_len-r: 140\ntruncation_err: 2\nquality_err: 2\n\n## Diversity metrics\nsampling_depth: 500\n</code></pre>"},{"location":"workflow/denoise-asv/","title":"Denoising and ASV","text":"<p>Sequencing data undergoes multiple processing steps before reaching the taxonomic analysis stage. Each of these steps can introduce a certain percentage of errors. This step focuses on correcting those sequencing errors and inferring exact amplicon sequence variants (ASVs) to improve data accuracy.</p> <p>Additionally, this step filters out low-quality reads and removes chimeric sequences, which are artifacts generated during PCR amplification.</p> <p>To achieve this, the DADA2 plugin in Qiime2 is used. DADA2 applies an error model-based approach to denoise reads, ensuring high-resolution and reliable microbiome analysis. <pre><code>##########################################################\n#                 DENOISE &amp; ASVs\n##########################################################\n\nrule dada2:\n  input:\n    q2_primerRM = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux-noprimer.qza\"\n  output:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\",\n    rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-rep-seqs.qza\",\n    stats = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-stats-dada2.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_dada2_q2.log\"\n  shell:\n    \"\"\"qiime dada2 denoise-paired \\\n        --i-demultiplexed-seqs {input.q2_primerRM} \\\n        --p-trunc-q {config[truncation_err]} \\\n        --p-trunc-len-f {config[truncation_len-f]} \\\n        --p-trunc-len-r {config[truncation_len-r]} \\\n        --o-table {output.table} \\\n        --o-representative-sequences {output.rep} \\\n        --o-denoising-stats {output.stats}\"\"\"\n\n\nrule dada2_stats:\n  input:\n    stats = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-stats-dada2.qza\"\n  output:\n    stats_viz = SCRATCH + \"/\" + OUTPUTDIR + \"/viz/\" + PROJ + \"-stats-dada2.qzv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_dada2-stats_q2.log\"\n  shell:\n   \"\"\"qiime metadata tabulate \\\n       --m-input-file {input.stats} \\\n       --o-visualization {output.stats_viz}\"\"\"\n</code></pre></p>"},{"location":"workflow/denoise-asv/#references","title":"References","text":"<ol> <li>Callahan, B. J., McMurdie, P. J., Rosen, M. J., Han, A. W., Johnson, A. J. A., &amp; Holmes, S. P. (2016). DADA2: High-resolution sample inference from Illumina amplicon data. Nature methods, 13(7), 581-583.</li> </ol>"},{"location":"workflow/diversity_metrics/","title":"Diversity metrics","text":"<p>This step computes multiple alpha and beta diversity measures to assess the bacterial composition identified in the sequencing reads.</p> <ul> <li> <p>Alpha diversity measures: These metrics estimate the richness (number of species) and evenness (distribution of species) within a single sample, providing insights into the diversity of microbial communities.</p> </li> <li> <p>Beta diversity measures: These metrics assess the similarity or dissimilarity of microbial compositions between different samples, offering insights into how microbial communities vary across samples.</p> </li> </ul>"},{"location":"workflow/diversity_metrics/#phylogenetic-tree-generation","title":"Phylogenetic tree generation","text":"<pre><code>rule phy_tree:\n  input:\n     rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-rep-seqs.qza\",\n  output:\n    aligned_seqs = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-aligned-rep-seqs.qza\",\n    aligned_masked = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-masked-aligned-rep-seqs.qza\",\n    unrooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-unrooted-tree.qza\",\n    rooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-rooted-tree.qza\",\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_phylogeneticTREE_q2.log\"\n\n  shell:\n    \"\"\"qiime phylogeny align-to-tree-mafft-fasttree \\\n        --i-sequences {input.rep} \\\n        --o-alignment {output.aligned_seqs} \\\n        --o-masked-alignment {output.aligned_masked} \\\n        --o-tree {output.unrooted_tree} \\\n        --o-rooted-tree {output.rooted_tree}\"\"\"\n</code></pre>"},{"location":"workflow/diversity_metrics/#diversity-metrics-computation","title":"Diversity metrics computation","text":"<pre><code>rule div_met:\n  input:\n     rooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-rooted-tree.qza\",\n     table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\"\n  output:\n     output_dir = directory(SCRATCH + \"/\" + OUTPUTDIR + \"/diversity\")\n\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_phylogeneticTREE_q2.log\"\n\n  shell:\n    \"\"\"qiime diversity core-metrics-phylogenetic \\\n        --i-phylogeny {input.rooted_tree} \\\n        --i-table {input.table} \\\n        --p-sampling-depth {SAMPLING_DEPTH} \\\n        --m-metadata-file {METADATA} \\\n        --output-dir {output.output_dir}\"\"\"\n</code></pre>"},{"location":"workflow/load_config/","title":"Loading configuration data in the workflow","text":"<p>This is the first step for creating our 16S amplicon processing workflow. We will first import our configuration file and extract project-related information. </p> <p>We simply specify our config file in a <code>key:value</code> format with key as <code>configfile</code>. Snakemake takes care of loading the file and extraction of all configuration in a <code>config</code> dictionary. </p> <pre><code>import os\nimport pathlib\n\n# Specify config file\nconfigfile: \"config.yaml\"\n\n##########################################################\n#                 SET CONFIG VARS\n##########################################################\n\nPROJ = config[\"project\"]\nSCRATCH = config[\"scratch\"]\nINPUTDIR = config[\"raw_data\"]\nOUTPUTDIR = config['outputDIR']\nMETADATA = config[\"metadata\"]\nSAMPLING_DEPTH= config['sampling_depth']\n\n# Fastq files naming config\nSUF = config['suffix']\nR1_SUF = str(config[\"r1_suf\"])\nR2_SUF = str(config[\"r2_suf\"])\n\n# Trimmomatic config\nTRIMM_PARAMS = config['trimm_params']\n\n# Database information\nDB = config[\"database\"]\nDB_classifier = config[\"database_classifier\"]\nDB_tax = config[\"database_tax\"]\n</code></pre>"},{"location":"workflow/load_qiime/","title":"Import in Qiime2","text":"<p>Our workflow uses the Qiime2 tool for processing read sequences. Qiime2 has its file formatting (.qza or .qzv), which requires storing read sequences in the Qiime2 file format. Qiime provides an interface for loading read sequences from different protocols (e.g., FASTQ data with the EMP protocol, Multiplexed FASTQ data). Each of these protocols stores files in a specific format.</p> <p>This step uses \"Fastq manifest\" format. it requires It requires a tab-separated values (TSV) file, known as a manifest file, which includes three columns:\"</p> <ul> <li><code>sample-id</code> \u2013 Unique sample identifier</li> <li><code>forward-absolute-filepath</code> \u2013 Full file path to forward reads</li> <li><code>reverse-absolute-filepath</code> \u2013 Full file path to reverse reads</li> </ul>"},{"location":"workflow/load_qiime/#creating-manifest-file","title":"Creating manifest file","text":"<p>This part of the workflow creates a manifest file with aforementioned three columns in TSV format. </p> <pre><code>##########################################################\n#                   CREATE MANIFEST FILE\n##########################################################\nrule create_manifest:\n    input:\n        r1 = expand(SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_paired.fastq.gz\",sample=SAMPLES),\n        r2 = expand(SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_paired.fastq.gz\",sample=SAMPLES),\n    output:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/\" + \"manifest.csv\"\n    params:\n        trim_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\",\n        out_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/\",\n        num_chars = 12\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/log/\" + \"qiime2/manifest.log\"\n\n    shell:\n        \"\"\"\n        python3 ../scripts/create_manifest.py --input {params.trim_dir} \\\n            --output {params.out_dir} \\\n            --num_chars {params.num_chars}\n        \"\"\"\n</code></pre>"},{"location":"workflow/load_qiime/#loading-into-qiime2","title":"Loading into qiime2","text":"<p>This part of the workflow utilizes the manifest file created by the above rule and load the trimmed read sequences in Qiime2 artifict (i.e., <code>.qza</code>). </p> <pre><code>##########################################################\n#                 LOAD DATA \n##########################################################\nrule import_qiime:\n  input:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/\" + \"manifest.csv\"\n  output:\n    q2_import = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_q2.log\"\n  params:\n    type=\"SampleData[PairedEndSequencesWithQuality]\",\n    input_format=\"PairedEndFastqManifestPhred33\",\n  shell:\n    \"\"\"\n    export TMPDIR={config[tmp_dir]}\n    qiime tools import \\\n    --type {params.type} \\\n    --input-path {input} \\\n    --output-path {output.q2_import} \\\n    --input-format {params.input_format} \n    \"\"\"\n</code></pre>"},{"location":"workflow/load_qiime/#references","title":"References","text":"<ol> <li>Bolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A., Abnet, C. C., Al-Ghalith, G. A., ... &amp; Caporaso, J. G. (2019). Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2. Nature biotechnology, 37(8), 852-857.</li> </ol>"},{"location":"workflow/phylogenetic_tree/","title":"Snakemake workflow","text":"<pre><code>graph TD\nA[Client] --&gt; B[Load Balancer]\nB --&gt; C[Server01]\nB --&gt; D[Server02]\n</code></pre>"},{"location":"workflow/quality_check/","title":"Quality check","text":"<p>The first step of the workflow is to perform quality check of raw sequence data. This check provides insights into the quality of reads which facilitate deciding over parameters such as length to trim. </p> <p>Here, we use <code>Fastqc</code> and <code>Multiqc</code> tools. Fastqc generates a report in html format for each sequence file. Multiqc summarises all those reports into a single file making it easier to comprehend various quality metrics for the entire dataset.</p>"},{"location":"workflow/quality_check/#file-names","title":"File names","text":"<p>To execute Fastqc for each sequence file, we need a way to have access to all filenames. It can be done manually. But Snakemake provides a wonderful utility called <code>glob_wildcards</code>. This utility automatically scans and fetch all file names following a given naming convention.</p> <p>For example, our dataset has filenames in a specific format (e.g., <code>Cancer1-2355_S61_L001_R1_001.fastq.gz</code>). Here, <code>Cancer1-2355_S61</code> is the sample identifier and <code>R1</code> is the direction of read. </p> <p>We can represent all those files using a single syntax <code>&lt;sampleid&gt;_L001_&lt;read&gt;_001.fastq.gz</code>. Here, <code>sampleid</code> is the identifier for the read; <code>read</code> is identifier for direction of read, i.e., R1, R2. </p> <p>To extract all sampleid and read, we simply write a single statement using <code>glob_wildcards</code> The following statement in our workflow scans the directory and fetch all sample and num information in two lists - SAMPLES AND NUMS.</p> <pre><code># global wild cards of sample and pairpair list\n(SAMPLES,NUMS) = glob_wildcards(\"/{sample}_L001_{num}_001.fastq.gz\")\n</code></pre>"},{"location":"workflow/quality_check/#fastqc-rule","title":"Fastqc rule","text":"<pre><code>##########################################################\n#                 FASTQC - QUALITY REPORTS\n##########################################################\nrule fastqc:\n    input:\n        SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_{num}\" + SUF\n    output:\n        html = SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.html\",\n        zip = SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.zip\",\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + \"fastqc/fastqc_{sample}_{num}.log\",\n    threads: 10\n    resources:\n        mem_mb = 1024\n    wrapper:\n        \"v5.5.2/bio/fastqc\"\n</code></pre>"},{"location":"workflow/quality_check/#multiqc-rule","title":"Multiqc rule","text":"<pre><code>##########################################################\n#                 MULTIQC - QUALITY REPORTS MERGE\n##########################################################\nrule multiqc:\n    input:\n        expand(SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.zip\", sample=SAMPLES,num=NUMS)\n    output:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/multiqc/multiqc_report.html\"\n    log:\n        SCRATCH + \"/logs\" + \"/multiqc/multiqc.log\"\n    params:\n        report_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/multiqc/\" \n    wrapper:\n        \"v1.31.1/bio/multiqc\"\n</code></pre>"},{"location":"workflow/quality_check/#references","title":"References","text":"<ol> <li>Andrews, S. (2010). FastQC:  A Quality Control Tool for High Throughput Sequence Data [Online]. Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</li> <li>Ewels, P., Magnusson, M., Lundin, S., &amp; K\u00e4ller, M. (2016). MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics, 32(19), 3047-3048.</li> </ol>"},{"location":"workflow/relative_abundance/","title":"Relative abundance computation","text":"<p>This step transforms Qiime2 artifacts containing ASVs (Amplicon Sequence Variants) and their associated taxonomies into TSV (Tab-Separated Values) format for downstream analysis. This conversion makes the data compatible with other analysis tools and workflows. <pre><code>##########################################################\n#                 RELATIVE FREQUENCY TABLE GENERATION\n##########################################################\nrule taxa_collapse:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\",\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  output:\n    table_phyla = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-phyla-table.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_taxa_collapse_q2.log\"\n  shell:\n    \"\"\"qiime taxa collapse \\\n      --i-table {input.table} \\\n      --i-taxonomy {input.sklearn} \\\n    --p-level 6 \\\n      --o-collapsed-table {output.table_phyla}\"\"\"\n\n\nrule rel_freq_table:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-phyla-table.qza\"\n  output:\n    rel_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-phyla-table.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_freq_q2.log\"\n  shell:\n    \"\"\"qiime feature-table relative-frequency \\\n     --i-table {input.table} \\\n     --o-relative-frequency-table {output.rel_table}\"\"\"\n\nrule rel_freq_table_biom:\n  input:\n    rel_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-phyla-table.qza\"\n  output:\n    biom_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/feature-table.biom\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/\")\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_freq_biom_q2.log\"\n  shell:\"\"\"qiime tools export \\\n     --input-path {input.rel_table} \\\n     --output-path {params}\n  \"\"\"\n\nrule biom_tsv:\n  input:\n    biom_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/feature-table.biom\"\n  output:\n    rel_table_tsv = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-freq-table.tsv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_tsv_q2.log\"\n  shell:\n    \"biom convert -i {input.biom_table} -o {output.rel_table_tsv} --to-tsv\"\n</code></pre></p>"},{"location":"workflow/taxonomy/","title":"Taxonomy generation","text":"<p>This step assigns taxonomic labels to the amplicon sequence variants (ASVs) obtained from the denoising step. To accomplish this, a prebuilt taxonomy classifier, trained on a reference dataset, is used. Multiple reference datasets are available for this task, such as Greengenes and Silva.</p> <p>The taxonomy classifier assigns taxonomic labels to each ASV, providing information on hierarchical levels such as Domain, Phylum, Class, Order, Genus, and Species.</p> <p>Important</p> <p>16S rRNA sequence analysis typically supports taxonomic resolution only up to the genus level.</p>"},{"location":"workflow/taxonomy/#classify-taxonomies","title":"Classify taxonomies","text":"<pre><code>##########################################################\n#                 TAXONOMIC ASSIGNMENT\n##########################################################\n\nrule assign_tax:\n  input:\n    rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rep-seqs.qza\",\n    db_classified = DB_classifier\n  output:\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_sklearn_q2.log\"\n  shell:\n    \"\"\"qiime feature-classifier classify-sklearn \\\n      --i-classifier {input.db_classified} \\\n      --i-reads {input.rep} \\\n      --o-classification {output.sklearn}\"\"\"\n</code></pre>"},{"location":"workflow/taxonomy/#convert-qiime2-artifact-to-table","title":"Convert Qiime2 artifact to table","text":"<pre><code>rule gen_table:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\"\n  output:\n    table_biom = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"feature-table.biom\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportBIOM_q2.log\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\")\n  shell:\n    \"qiime tools export --input-path {input.table} --output-path {params}\"\n\nrule convert:\n  input:\n    table_biom = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"feature-table.biom\"\n  output:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.tsv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportTSV_q2.log\"\n  shell:\n    \"biom convert -i {input} -o {output} --to-tsv\"\n\nrule gen_tax:\n  input:\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  output:\n     table_tax = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\"  + \"taxonomy.tsv\",\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportTAXTSV_q2.log\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\")\n  shell:\n    \"qiime tools export --input-path {input.sklearn} --output-path {params}\"\n</code></pre>"},{"location":"workflow/trimmomatic/","title":"Trimmomatic","text":"<p>This is the step where the read sequences are trimmed to ensure a high-quality data for subsequent analysis. More details on trimmomatic can be found here.</p> <p>We use Snakemake wrapper for trimmomatic.</p> <pre><code>##########################################################\n#                 TRIMMOMATIC\n##########################################################\nrule trimmomatic:\n    input:\n        r1 = SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_R1_001.fastq.gz\",\n        r2 = SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_R2_001.fastq.gz\",\n    output:\n        r1 = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_paired.fastq.gz\",\n        r2 = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_paired.fastq.gz\",\n\n        r1_unpaired = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_unpaired.fastq.gz\",\n        r2_unpaired = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_unpaired.fastq.gz\",\n    threads: 20\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + \"trimmomatic/{sample}.log\"\n    params:\n        trimmer=[str(config['trimm_params'])]\n    wrapper:\n        \"v5.5.2/bio/trimmomatic/pe\"\n</code></pre>"},{"location":"workflow/trimmomatic/#references","title":"References","text":"<ol> <li>Bolger, A. M., Lohse, M., &amp; Usadel, B. (2014). Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics, 30(15), 2114-2120.</li> </ol>"}]}