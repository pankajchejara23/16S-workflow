{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A complete 16S amplicon sequence analysis workflow","text":"<p>This website provides a detailed explaination of automating the analysis of 16S amplicon read sequences using Snakemake. A complete analysis of 16S amplicon read sequences consists of several steps, ranging from quality control, taxonomy generation to statistical analysis. These steps must be executed in a specific order. </p> <p>To automate the execution of all analysis steps for 16S amplicon data, we use a workflow manager such as Snakemake. In this tutorial, we will demonstrate how to use Snakemake to create a workflow that reads raw 16S amplicon sequences and produces relative abundace data for subsequent analysis. </p>"},{"location":"dataset/","title":"16S Amplicon Sequence Reads","text":"<p>For this tutorial, we use 16S amplicon read sequences obtained from faecal sample from three patient groups:Healthy,Adenoma,CRC (Zackular et al., 2014). The authors investigated gut microbiome potential for predicting cases with colorectal cancer. They used 16S amplicon sequences for the analysis. You can read more about the study here. </p>"},{"location":"dataset/#data-files","title":"Data files","text":"<p>You can download the 16S amplicon sequence reads and metadata file from the following links.</p> <ul> <li>Raw Fastq Paired Reads</li> <li>Metadata</li> </ul>"},{"location":"dataset/#references","title":"References","text":"<ol> <li>Zackular, J. P., Rogers, M. A., Ruffin IV, M. T., &amp; Schloss, P. D. (2014). The human gut microbiome as a screening tool for colorectal cancer. Cancer prevention research, 7(11), 1112-1121.</li> </ol>"},{"location":"execution/","title":"Execution","text":"<p>This section guides you through running the workflow to compute relative abundance metrics for various microbial species.</p>"},{"location":"execution/#clone-the-repository","title":"Clone the repository","text":"<p>Clone the GitHub repository to access the scripts and configuration files needed for the workflow: <pre><code>git clone https://github.com/pankajchejara23/16S-workflow\n</code></pre></p>"},{"location":"execution/#download-dataset","title":"Download dataset","text":"<p>Navigate to the cloned repository and download the dataset using the provided script: <pre><code>cd 16S-workflow\nsh ./scripts/downloader.sh\n</code></pre></p>"},{"location":"execution/#create-processing-environment","title":"Create processing environment","text":"<p>Set up the required Conda environment using the environment file: <pre><code>conda env create -f environment.yml\nconda activate qiime2_snakemake\n</code></pre></p>"},{"location":"execution/#configure-snakemake-workflow","title":"Configure Snakemake workflow","text":"<p>The <code>config.yaml</code> file is preconfigured for this workflow. If needed, edit the file to customize parameters for your specific use case.</p>"},{"location":"execution/#execute","title":"Execute","text":"<p>Run the Snakemake workflow using the following command: <pre><code>Snakemake --cores 4 \n</code></pre></p>"},{"location":"setup/","title":"Setup","text":"<p>We start by setting up our machine with required tools and packages to get it ready for executing the workflow. We primarily need the following tools</p> <ul> <li>Miniconda</li> <li>Qiime2 (2024.10)</li> <li>Snakemake</li> </ul> <p>To speed up the installation, we will use an <code>environment.yml</code> file exported from a conda environment that successfully executed the workflow. You can download the file from environment. </p> <p>You can create a new environment using the following command. </p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Note</p> <p>To execute this command you need to have either Miniconda or Anaconda installed on your system. </p>"},{"location":"workflow/","title":"Snakemake workflow","text":"<p>This section provides detailed explanation for building a complete workflow using Snakemake. For our workflow, we will do the following.</p> <ol> <li>Configuration file: We will create a <code>config.yml</code> file which will contain information in <code>key</code>:<code>value</code> format about the project such as input directory of raw reads file and paths to reference databases.  Storing this information in a seperate configuration file allows us to reuse our workflow for different projects. </li> <li>Write rules for each step: We will write rules following Snakemake syntax for each step. These rules basically invovles specifying input file, output file, command to execute and other parameters. </li> <li>Write <code>rule_all</code>: By default Snakemake executes the first rule in the workflow. If the input files required for that rule are generated by other rules, those rules are also executed as well. To ensure that Snakemake executes all rules, we will write a <code>rule_all</code> that specifies all output files from all rules as inputs.</li> </ol> <p>Don't worry if it does not make sense now -- we will go through this in detail in the following sections.</p>"},{"location":"workflow/adapters/","title":"Adapter removal","text":"<pre><code>##########################################################\n#                 REMOVE PRIMERS\n##########################################################\n\nrule rm_primers:\n  input:\n    q2_import = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux.qza\"\n  output:\n    q2_primerRM = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ +  \"-PE-demux-noprimer.qza\"\n  log:\n     SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_primer_q2.log\"\n\n  shell:\n    \"\"\"qiime cutadapt trim-paired \\\n       --i-demultiplexed-sequences {input.q2_import} \\\n       --p-front-f {config[primerF]} \\\n       --p-front-r {config[primerR]} \\\n       --p-error-rate {config[primer_err]} \\\n       --p-overlap {config[primer_overlap]} \\\n       --o-trimmed-sequences {output.q2_primerRM}\n       \"\"\"\n</code></pre>"},{"location":"workflow/config/","title":"Config file","text":"<p>A configuration file specifies all project-related inforamation in a yml file. This configuration file is all which needs to be updated in order to execute the workflow for another project. This makes bioinfomatician's file easier. Just think chaning your workflow every time you run it for a different project. In this cases, there is a high likelihood of human error. </p> <p>To avoid this situation, all project-related information are kept in a yml file. Our workflow uses the following configuration file.</p> <p>The table provides information on each key and thier meaning. </p> Key Description project Name of the project; serves as an identifier for the analysis. scratch Path to the scratch or working directory where outputs will be stored. raw_data Directory containing raw input data files (e.g., raw FASTQ files). outputDIR Directory where final pipeline outputs will be stored. metadata Path to the QIIME2 metadata file containing sample information such as groups or treatments. tmp_dir Temporary directory for QIIME2 operations (e.g., caching files during execution). suffix File suffix used to identify the raw FASTQ files (e.g., <code>_001.fastq.gz</code>). prefix File prefix used in raw FASTQ file naming (e.g., <code>_L001_</code>). r1_suf Identifier used for forward read files (R1) in paired-end sequencing data. r2_suf Identifier used for reverse read files (R2) in paired-end sequencing data. threads Number of threads to use for multi-threaded tools like Trimmomatic. trimm_params Parameters for trimming raw sequences with Trimmomatic (e.g., cropping sequences to a length of 200). primerF Forward primer sequence used for amplifying the 16S rRNA region. primerR Reverse primer sequence used for amplifying the 16S rRNA region. primer_err Maximum allowable primer error rate during sequence matching. primer_overlap Minimum overlap between primer and sequence for matching. database Path to the QIIME2 reference database sequences file (e.g., SILVA 138.1). database_classifier Path to the QIIME2 pre-trained classifier for taxonomic classification. database_tax Path to the QIIME2 taxonomy file associated with the reference database. truncation_err Maximum allowable error for sequence truncation in DADA2 (used for quality filtering). truncation_len-f Length to truncate forward reads (R1) in DADA2 analysis. truncation_len-r Length to truncate reverse reads (R2) in DADA2 analysis. quality_err Maximum allowable quality error for sequence processing in DADA2. sampling_depth Depth of subsampling used for diversity analysis to ensure consistency across samples. <p>The following is the configuration file used for the workflow.</p> <pre><code># Basic setup\nproject: crc_microbiome\nscratch: /Users/pankaj/Documents/Metrosert/public_dataset/zakular2014\nraw_data: partial_raw_data\noutputDIR: full_pipeline\nmanifest: manifest_partial.csv\nmetadata: metadata_partial.tsv\n\n# Tmp direcotry for qiime2\ntmp_dir: ./\n\n# Fastq file naming config\nsuffix: _001.fastq.gz\nprefix: _L001_\nr1_suf: R1\nr2_suf: R2\n\n# Trimmomatic config\nthreads: 20\ntrimm_params: CROP:200\n\n## 16S adapters from Zackular et al., 2014\nprimerF: GTGCCAGCMGCCGCGGTAA\nprimerR: GGACTACHVGGGTWTCTAAT\nprimer_err: 0.4\nprimer_overlap: 3\n\n## Reference database\ndatabase: silva-138.1-ssu-nr99-seqs-515f-806r.qza\ndatabase_classifier: silva_classifier.qza\ndatabase_tax: silva-138.1-ssu-nr99-tax.qza\n\n## DADA2 - ASV flags\ntruncation_err: 2\ntruncation_len-f: 150\ntruncation_len-r: 140\ntruncation_err: 2\nquality_err: 2\n\n## Diversity metrics\nsampling_depth: 500\n</code></pre>"},{"location":"workflow/denoise-asv/","title":"Denoising and ASV","text":"<pre><code>##########################################################\n#                 DENOISE &amp; ASVs\n##########################################################\n\nrule dada2:\n  input:\n    q2_primerRM = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux-noprimer.qza\"\n  output:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\",\n    rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-rep-seqs.qza\",\n    stats = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-stats-dada2.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_dada2_q2.log\"\n  shell:\n    \"\"\"qiime dada2 denoise-paired \\\n        --i-demultiplexed-seqs {input.q2_primerRM} \\\n        --p-trunc-q {config[truncation_err]} \\\n        --p-trunc-len-f {config[truncation_len-f]} \\\n        --p-trunc-len-r {config[truncation_len-r]} \\\n        --o-table {output.table} \\\n        --o-representative-sequences {output.rep} \\\n        --o-denoising-stats {output.stats}\"\"\"\n\n\nrule dada2_stats:\n  input:\n    stats = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-stats-dada2.qza\"\n  output:\n    stats_viz = SCRATCH + \"/\" + OUTPUTDIR + \"/viz/\" + PROJ + \"-stats-dada2.qzv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_dada2-stats_q2.log\"\n  shell:\n   \"\"\"qiime metadata tabulate \\\n       --m-input-file {input.stats} \\\n       --o-visualization {output.stats_viz}\"\"\"\n</code></pre>"},{"location":"workflow/diversity_metrics/","title":"Diversity metrics","text":""},{"location":"workflow/diversity_metrics/#phylogenetic-tree-generation","title":"Phylogenetic tree generation","text":"<pre><code>rule phy_tree:\n  input:\n     rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-rep-seqs.qza\",\n  output:\n    aligned_seqs = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-aligned-rep-seqs.qza\",\n    aligned_masked = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-masked-aligned-rep-seqs.qza\",\n    unrooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-unrooted-tree.qza\",\n    rooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-rooted-tree.qza\",\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_phylogeneticTREE_q2.log\"\n\n  shell:\n    \"\"\"qiime phylogeny align-to-tree-mafft-fasttree \\\n        --i-sequences {input.rep} \\\n        --o-alignment {output.aligned_seqs} \\\n        --o-masked-alignment {output.aligned_masked} \\\n        --o-tree {output.unrooted_tree} \\\n        --o-rooted-tree {output.rooted_tree}\"\"\"\n</code></pre>"},{"location":"workflow/diversity_metrics/#diversity-metrics-computation","title":"Diversity metrics computation","text":"<pre><code>rule div_met:\n  input:\n     rooted_tree = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"tree/\" + PROJ + \"-rooted-tree.qza\",\n     table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\"\n  output:\n     output_dir = directory(SCRATCH + \"/\" + OUTPUTDIR + \"/diversity\")\n\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_phylogeneticTREE_q2.log\"\n\n  shell:\n    \"\"\"qiime diversity core-metrics-phylogenetic \\\n        --i-phylogeny {input.rooted_tree} \\\n        --i-table {input.table} \\\n        --p-sampling-depth {SAMPLING_DEPTH} \\\n        --m-metadata-file {METADATA} \\\n        --output-dir {output.output_dir}\"\"\"\n</code></pre>"},{"location":"workflow/load_config/","title":"Loading configuration data in the workflow","text":"<p>This is the first step for creating our 16S amplicon processing workflow. We will first import our configuration file and extract project-related information. </p> <p>We simply specify our config file in a <code>key:value</code> format with key as <code>configfile</code>. Snakemake takes care of loading the file and extraction of all configuration in a <code>config</code> dictionary. </p> <pre><code>import os\nimport pathlib\n\n# Specify config file\nconfigfile: \"config.yaml\"\n\n##########################################################\n#                 SET CONFIG VARS\n##########################################################\n\nPROJ = config[\"project\"]\nSCRATCH = config[\"scratch\"]\nINPUTDIR = config[\"raw_data\"]\nOUTPUTDIR = config['outputDIR']\nMETADATA = config[\"metadata\"]\nSAMPLING_DEPTH= config['sampling_depth']\n\n# Fastq files naming config\nSUF = config['suffix']\nR1_SUF = str(config[\"r1_suf\"])\nR2_SUF = str(config[\"r2_suf\"])\n\n# Trimmomatic config\nTRIMM_PARAMS = config['trimm_params']\n\n# Database information\nDB = config[\"database\"]\nDB_classifier = config[\"database_classifier\"]\nDB_tax = config[\"database_tax\"]\n</code></pre>"},{"location":"workflow/load_qiime/","title":"Import in Qiime2","text":""},{"location":"workflow/load_qiime/#creating-manifest-file","title":"Creating manifest file","text":"<pre><code>##########################################################\n#                   CREATE MANIFEST FILE\n##########################################################\nrule create_manifest:\n    input:\n        r1 = expand(SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_paired.fastq.gz\",sample=SAMPLES),\n        r2 = expand(SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_paired.fastq.gz\",sample=SAMPLES),\n    output:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/\" + \"manifest.csv\"\n    params:\n        trim_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\",\n        out_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/\",\n        num_chars = 12\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/log/\" + \"qiime2/manifest.log\"\n\n    shell:\n        \"\"\"\n        python3 ../scripts/create_manifest.py --input {params.trim_dir} \\\n            --output {params.out_dir} \\\n            --num_chars {params.num_chars}\n        \"\"\"\n</code></pre>"},{"location":"workflow/load_qiime/#loading-into-qiime2","title":"Loading into qiime2","text":"<pre><code>##########################################################\n#                 LOAD DATA \n##########################################################\nrule import_qiime:\n  input:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/\" + \"manifest.csv\"\n  output:\n    q2_import = SCRATCH + \"/\" + OUTPUTDIR +\"/\" + PROJ + \"-PE-demux.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_q2.log\"\n  params:\n    type=\"SampleData[PairedEndSequencesWithQuality]\",\n    input_format=\"PairedEndFastqManifestPhred33\",\n  shell:\n    \"\"\"\n    export TMPDIR={config[tmp_dir]}\n    qiime tools import \\\n    --type {params.type} \\\n    --input-path {input} \\\n    --output-path {output.q2_import} \\\n    --input-format {params.input_format} \n    \"\"\"\n</code></pre>"},{"location":"workflow/phylogenetic_tree/","title":"Snakemake workflow","text":"<pre><code>graph TD\nA[Client] --&gt; B[Load Balancer]\nB --&gt; C[Server01]\nB --&gt; D[Server02]\n</code></pre>"},{"location":"workflow/quality_check/","title":"Quality check","text":"<p>The first step of the workflow is to perform quality check of raw sequence data. This check provides insights into the quality of reads which facilitate deciding over parameters such as length to trim. </p> <p>Here, we use <code>Fastqc</code> and <code>Multiqc</code> tools. Fastqc generates a report in html format for each sequence file. Multiqc summarises all those reports into a single file making it easier to comprehend various quality metrics for the entire dataset.</p>"},{"location":"workflow/quality_check/#file-names","title":"File names","text":"<p>To execute Fastqc for each sequence file, we need a way to have access to all filenames. It can be done manually. But Snakemake provides a wonderful utility called <code>glob_wildcards</code>. This utility automatically scans and fetch all file names following a given naming convention.</p> <p>For example, our dataset has filenames in a specific format (e.g., <code>Cancer1-2355_S61_L001_R1_001.fastq.gz</code>). Here, <code>Cancer1-2355_S61</code> is the sample identifier and <code>R1</code> is the direction of read. </p> <p>We can represent all those files using a single syntax <code>&lt;sampleid&gt;_L001_&lt;read&gt;_001.fastq.gz</code>. Here, <code>sampleid</code> is the identifier for the read; <code>read</code> is identifier for direction of read, i.e., R1, R2. </p> <p>To extract all sampleid and read, we simply write a single statement using <code>glob_wildcards</code> The following statement in our workflow scans the directory and fetch all sample and num information in two lists - SAMPLES AND NUMS.</p> <pre><code># global wild cards of sample and pairpair list\n(SAMPLES,NUMS) = glob_wildcards(\"/{sample}_L001_{num}_001.fastq.gz\")\n</code></pre>"},{"location":"workflow/quality_check/#fastqc-rule","title":"Fastqc rule","text":"<pre><code>##########################################################\n#                 FASTQC - QUALITY REPORTS\n##########################################################\nrule fastqc:\n    input:\n        SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_{num}\" + SUF\n    output:\n        html = SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.html\",\n        zip = SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.zip\",\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + \"fastqc/fastqc_{sample}_{num}.log\",\n    threads: 10\n    resources:\n        mem_mb = 1024\n    wrapper:\n        \"v5.5.2/bio/fastqc\"\n</code></pre>"},{"location":"workflow/quality_check/#multiqc-rule","title":"Multiqc rule","text":"<pre><code>##########################################################\n#                 MULTIQC - QUALITY REPORTS MERGE\n##########################################################\nrule multiqc:\n    input:\n        expand(SCRATCH + \"/\" + OUTPUTDIR + \"/fastqc/\" + \"{sample}_L001_{num}_fastqc.zip\", sample=SAMPLES,num=NUMS)\n    output:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/multiqc/multiqc_report.html\"\n    log:\n        SCRATCH + \"/logs\" + \"/multiqc/multiqc.log\"\n    params:\n        report_dir = SCRATCH + \"/\" + OUTPUTDIR + \"/multiqc/\" \n    wrapper:\n        \"v1.31.1/bio/multiqc\"\n</code></pre>"},{"location":"workflow/relative_abundance/","title":"Relative abundance computation","text":"<pre><code>##########################################################\n#                 RELATIVE FREQUENCY TABLE GENERATION\n##########################################################\nrule taxa_collapse:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\",\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  output:\n    table_phyla = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-phyla-table.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_taxa_collapse_q2.log\"\n  shell:\n    \"\"\"qiime taxa collapse \\\n      --i-table {input.table} \\\n      --i-taxonomy {input.sklearn} \\\n    --p-level 6 \\\n      --o-collapsed-table {output.table_phyla}\"\"\"\n\n\nrule rel_freq_table:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-phyla-table.qza\"\n  output:\n    rel_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-phyla-table.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_freq_q2.log\"\n  shell:\n    \"\"\"qiime feature-table relative-frequency \\\n     --i-table {input.table} \\\n     --o-relative-frequency-table {output.rel_table}\"\"\"\n\nrule rel_freq_table_biom:\n  input:\n    rel_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-phyla-table.qza\"\n  output:\n    biom_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/feature-table.biom\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/\")\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_freq_biom_q2.log\"\n  shell:\"\"\"qiime tools export \\\n     --input-path {input.rel_table} \\\n     --output-path {params}\n  \"\"\"\n\nrule biom_tsv:\n  input:\n    biom_table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"rel-table/feature-table.biom\"\n  output:\n    rel_table_tsv = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rel-freq-table.tsv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_rel_tsv_q2.log\"\n  shell:\n    \"biom convert -i {input.biom_table} -o {output.rel_table_tsv} --to-tsv\"\n</code></pre>"},{"location":"workflow/taxonomy/","title":"Taxonomy generation","text":""},{"location":"workflow/taxonomy/#classify-taxonomies","title":"Classify taxonomies","text":"<pre><code>##########################################################\n#                 TAXONOMIC ASSIGNMENT\n##########################################################\n\nrule assign_tax:\n  input:\n    rep = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-rep-seqs.qza\",\n    db_classified = DB_classifier\n  output:\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ +  \"_sklearn_q2.log\"\n  shell:\n    \"\"\"qiime feature-classifier classify-sklearn \\\n      --i-classifier {input.db_classified} \\\n      --i-reads {input.rep} \\\n      --o-classification {output.sklearn}\"\"\"\n</code></pre>"},{"location":"workflow/taxonomy/#convert-qiime2-artifact-to-table","title":"Convert Qiime2 artifact to table","text":"<pre><code>rule gen_table:\n  input:\n    table = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.qza\"\n  output:\n    table_biom = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"feature-table.biom\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportBIOM_q2.log\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\")\n  shell:\n    \"qiime tools export --input-path {input.table} --output-path {params}\"\n\nrule convert:\n  input:\n    table_biom = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + \"feature-table.biom\"\n  output:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" + PROJ + \"-asv-table.tsv\"\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportTSV_q2.log\"\n  shell:\n    \"biom convert -i {input} -o {output} --to-tsv\"\n\nrule gen_tax:\n  input:\n    sklearn = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\" +  PROJ + \"-tax_sklearn.qza\"\n  output:\n     table_tax = SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\"  + \"taxonomy.tsv\",\n  log:\n    SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + PROJ + \"_exportTAXTSV_q2.log\"\n  params:\n    directory(SCRATCH + \"/\" + OUTPUTDIR + \"/asv/\")\n  shell:\n    \"qiime tools export --input-path {input.sklearn} --output-path {params}\"\n</code></pre>"},{"location":"workflow/trimmomatic/","title":"Trimmomatic","text":"<pre><code>##########################################################\n#                 TRIMMOMATIC\n##########################################################\nrule trimmomatic:\n    input:\n        r1 = SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_R1_001.fastq.gz\",\n        r2 = SCRATCH + \"/\" + INPUTDIR + \"/\" + \"{sample}_L001_R2_001.fastq.gz\",\n    output:\n        r1 = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_paired.fastq.gz\",\n        r2 = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_paired.fastq.gz\",\n\n        r1_unpaired = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R1_unpaired.fastq.gz\",\n        r2_unpaired = SCRATCH + \"/\" + OUTPUTDIR + \"/trim/\" +\"{sample}_R2_unpaired.fastq.gz\",\n    threads: 20\n    log:\n        SCRATCH + \"/\" + OUTPUTDIR + \"/logs/\" + \"trimmomatic/{sample}.log\"\n    params:\n        trimmer=[str(config['trimm_params'])]\n    wrapper:\n        \"v5.5.2/bio/trimmomatic/pe\"\n</code></pre>"}]}